package basic

/*
复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？
复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度

事后统计法：把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。其局限性如下
	测试结果非常依赖测试环境
		i9 处理器要比 i3 处理器执行的速度快很多
	测试结果受数据规模的影响很大
		对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别
		对于小规模的数据排序，插入排序可能反倒会比快速排序要快
大O复杂度表示法
	简介
		从 CPU 的角度来看，代码的每一行都执行着类似的操作：读数据-运算-写数据
		可以假设每行代码执行的时间都一样，为 unit_time
		所有代码的执行时间 T(n) 与每行代码的执行次数 f(n) 成正比
	T(n)=O(f(n))
		T(n)：代码执行的时间
		n：数据规模的大小
		f(n)：每行代码执行的次数总和
		公式中的 O，表示代码的执行时间 T(n) 与 f(n) 表达式成正比
	大 O 时间复杂度表示法
		实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势
		所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度
	忽略：公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略
		低阶
		常量
		系数

时间复杂度分析：全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系
	1.只关注循环执行次数最多的一段代码
	2.加法法则：总复杂度等于量级最大的那段代码的复杂度
		即便代码循环 10000 次、100000 次，只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间
		时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，都可以忽略掉
		因为它本身对增长趋势并没有影响
		如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n)))
	3.乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
		如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))
非多项式量级（只有两个）
	O(2^n)：指数阶
	O(n!)：阶乘阶

	把时间复杂度为非多项式量级的算法问题叫做 NP（Non-Deterministic Polynomial，非确定多项式）问题
	非多项式时间复杂度的算法其实是非常低效的算法
多项式量级
	O(1)：常量阶
	O(log n)：对数阶
	O(n)：线性阶
	O(n log n)：线性对数阶
	O(n^2)：平方阶
	O(n^3)：立方阶
	...
	O(n^k)：k次方阶

	不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)
	log3n 就等于 log32 * log2n，所以 O(log3n) = O(C * log2n)，其中 C=log32 是一个常量
递归时间复杂度求解方法
	递推公式
	递归树
// TODO

空间复杂度分析：全称是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系
	除了原本的数据存储空间外，算法运行还需要额外的存储空间

最好情况时间复杂度（best case time complexity）
	在最理想的情况下，执行这段代码的时间复杂度
最坏情况时间复杂度（worst case time complexity）
	在最糟糕的情况下，执行这段代码的时间复杂度
平均情况时间复杂度（average case time complexity）
	加权平均时间复杂度 或者 期望时间复杂度
均摊时间复杂度（amortized time complexity）
	平均复杂度只在某些特殊情况下才会用到
	而均摊时间复杂度应用的场景比它更加特殊、更加有限，是一种特殊的平均时间复杂度
	分析方法：摊还分析（或者叫平摊分析）
*/

// add 示例函数，仅用于分析均摊时间复杂度
func add_02(v int) {
	if i_02 >= l_02 {
		l_02 <<= 1
		nArr := make([]int, l_02)
		copy(nArr, arr_02)
		arr_02 = nArr
	}
	arr_02[i_02] = v
	i_02++
}

var (
	arr_02 = make([]int, 1)
	i_02   int
	l_02   = len(arr_02)
)
